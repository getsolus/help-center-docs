"use strict";(self.webpackChunksolus_help_center=self.webpackChunksolus_help_center||[]).push([[7135],{83888:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>d});var s=n(85893),t=n(11151);const l={title:"Intro to Optimizing Packages on Solus",description:"Explore how to employ advanced compiler techniques such as PGO, BOLT & Glibc HWCaps to squeeze extra performance from packages using libwebp as a test vehicle",slug:"solus-optimizing-packages",authors:[{name:"Joey Riches",title:"Solus Staff",url:"https://github.com/joebonrichie",image_url:"https://avatars.githubusercontent.com/u/5338090?s=400&u=f77ed45c7e83814ce3e8bd199fc293bd5b53682b&v=4"}],tags:["pgo","lto","solus","packaging","optimization",3,"clang","gnu","llvm","glibc","hwcaps","x86_64-v3"],hide_table_of_contents:!1},r="Cual es la causa",a={permalink:"/blog/solus-optimizing-packages",source:"@site/devlog/2024-02-09-Intro-to-optimizing-packages-on-solus.md",title:"Intro to Optimizing Packages on Solus",description:"Explore how to employ advanced compiler techniques such as PGO, BOLT & Glibc HWCaps to squeeze extra performance from packages using libwebp as a test vehicle",date:"2024-02-09T00:00:00.000Z",tags:[{label:"pgo",permalink:"/blog/tags/pgo"},{label:"lto",permalink:"/blog/tags/lto"},{label:"solus",permalink:"/blog/tags/solus"},{label:"packaging",permalink:"/blog/tags/packaging"},{label:"optimization",permalink:"/blog/tags/optimization"},{label:"3",permalink:"/blog/tags/3"},{label:"clang",permalink:"/blog/tags/clang"},{label:"gnu",permalink:"/blog/tags/gnu"},{label:"llvm",permalink:"/blog/tags/llvm"},{label:"glibc",permalink:"/blog/tags/glibc"},{label:"hwcaps",permalink:"/blog/tags/hwcaps"},{label:"x86_64-v3",permalink:"/blog/tags/x-86-64-v-3"}],readingTime:22.32,hasTruncateMarker:!0,authors:[{name:"Joey Riches",title:"Solus Staff",url:"https://github.com/joebonrichie",image_url:"https://avatars.githubusercontent.com/u/5338090?s=400&u=f77ed45c7e83814ce3e8bd199fc293bd5b53682b&v=4",imageURL:"https://avatars.githubusercontent.com/u/5338090?s=400&u=f77ed45c7e83814ce3e8bd199fc293bd5b53682b&v=4"}],frontMatter:{title:"Intro to Optimizing Packages on Solus",description:"Explore how to employ advanced compiler techniques such as PGO, BOLT & Glibc HWCaps to squeeze extra performance from packages using libwebp as a test vehicle",slug:"solus-optimizing-packages",authors:[{name:"Joey Riches",title:"Solus Staff",url:"https://github.com/joebonrichie",image_url:"https://avatars.githubusercontent.com/u/5338090?s=400&u=f77ed45c7e83814ce3e8bd199fc293bd5b53682b&v=4",imageURL:"https://avatars.githubusercontent.com/u/5338090?s=400&u=f77ed45c7e83814ce3e8bd199fc293bd5b53682b&v=4"}],tags:["pgo","lto","solus","packaging","optimization","3","clang","gnu","llvm","glibc","hwcaps","x86_64-v3"],hide_table_of_contents:!1},unlisted:!1,nextItem:{title:"Don't call me MATE, pal!",permalink:"/blog/don't-call-me-mate-pal"}},o={authorsImageUrls:[void 0]},d=[{value:"Let&#39;s Start Obvious",id:"lets-start-obvious",level:2},{value:"PGO is great, except, when it isn&#39;t.",id:"pgo-is-great-except-when-it-isnt",level:2},{value:"256 Vector Units go brrrrrr...",id:"256-vector-units-go-brrrrrr",level:2},{value:"Something about <code>perf</code> to the rescue",id:"something-about-perf-to-the-rescue",level:2},{value:"Choosing Wisely",id:"choosing-wisely",level:2},{value:"Adjusting the Benchmark",id:"adjusting-the-benchmark",level:2},{value:"Partial Profiling",id:"partial-profiling",level:3},{value:"I Zee a Purty lil&#39; Package",id:"i-zee-a-purty-lil-package",level:2},{value:"Squeezing more from zlib-ng",id:"squeezing-more-from-zlib-ng",level:2}];function c(e){const i={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(i.p,{children:["We'll explore how to build packages with advanced compiler techniques in order to squeeze more performance out of the box for packages in Solus. We'll be using the story of how ",(0,s.jsx)(i.code,{children:"libwebp"})," was optimized for and how it led to an unexpected side quest."]}),"\n",(0,s.jsx)(i.p,{children:"Linux distributions have a lot of control over how a source-based package gets compiled and shipped to users as part of a binary repository. Aggressive and advanced compiler optimization techniques, as well as other methods can be used to provide greater out of the box performance for end users. This can greatly benefit users running on older hardware to provide a snappier end-user experience; reducing time waiting on a heavy workload to finish; or even improved battery life; amongst other improvements."}),"\n",(0,s.jsxs)(i.p,{children:["Part of the problem is, a packager's time is limited. So how, as a distribution, do you choose to try provide faster compatible packages for an end user. A historic approach is to simply change the default compiler flags for ",(0,s.jsx)(i.em,{children:"all"})," packages, such as enabling ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Interprocedural_optimization",children:"LTO"})," by default. Whilst this approach can work well, at Solus the philosophy is slightly different where a packager can trivially enable several advanced compiler optimization techniques such as ",(0,s.jsx)(i.a,{href:"https://en.wikipedia.org/wiki/Profile-guided_optimization",children:"PGO"})," without too much faffing around on a ",(0,s.jsx)(i.em,{children:"targeted"})," package."]}),"\n",(0,s.jsx)(i.p,{children:"The benefits of such an approach are:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Can target the performance of a specific package to benefit ",(0,s.jsx)(i.em,{children:"all"})," users"]}),"\n",(0,s.jsx)(i.li,{children:"A compiler optimization may improve one package, but may not apply globally to all packages."}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The downsides are such:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Requires additional packager time to benchmark and experiment with different optimization strategies."}),"\n",(0,s.jsxs)(i.li,{children:["Requires the packager to ",(0,s.jsx)(i.em,{children:"choose"})," and invest time into improving performance of a package."]}),"\n",(0,s.jsx)(i.li,{children:"Requires the packager to find an appropriate benchmark to test the package against."}),"\n",(0,s.jsx)(i.li,{children:"Experimenting with compiler optimizations may not bear fruit: no meaningful improvement in performance, or there may be some other bottleneck."}),"\n"]}),"\n",(0,s.jsx)(i.h1,{id:"optimization-techniques-available",children:"Optimization Techniques Available"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["speed:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["As simple as it gets really, build a package with ",(0,s.jsx)(i.code,{children:"-O3"})," instead of ",(0,s.jsx)(i.code,{children:"-O2"})," as well as any other flags deemed worthy to be included as part of the ",(0,s.jsx)(i.code,{children:"speed"})," flags. The main drawback of this is that ",(0,s.jsx)(i.code,{children:"-O3"})," is not guaranteed to produce faster results than building with ",(0,s.jsx)(i.code,{children:"-O2"})," and typically will produce bigger binaries. The days of ",(0,s.jsx)(i.code,{children:"-O3"})," outright breaking your program in weird unexpected ways is largely behind us."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["LTO:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Compared to some other distributions ",(0,s.jsx)(i.code,{children:"-flto"})," is not yet enabled by default on Solus. LTO is almost guaranteed to provide a %1 or slightly larger performance improvement as well as a smaller binary at the cost of increased compiling times and memory usage during build. When combined with other optimization techniques such as PGO the LTO optimization can really stretch its legs and provide even greater uplift!"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["Clang:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Not strictly an optimization, but, building a package with ",(0,s.jsx)(i.code,{children:"clang"})," instead of ",(0,s.jsx)(i.code,{children:"gcc"})," and ",(0,s.jsx)(i.code,{children:"ld.ldd"})," to link instead of the infamous ",(0,s.jsx)(i.code,{children:"ld.bfd"})," may provide a faster package out of the box. You'll have to be careful of subtle ABI differences if building with ",(0,s.jsx)(i.code,{children:"clang"}),". If in doubt, and, ",(0,s.jsx)(i.code,{children:"clang"})," is the obvious choice, perform safety rebuilds on all reverse dependencies of the package."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["PGO:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Profile guided optimization. Build once with instrumentation in order to collect profile data when ran. Run the program using a representative workload in order to collect profiling data. Build the program again with the profiling data provided in order to build an optimized variant."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["BOLT:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:['Binary optimization and layout tool. You can think of this as "post-link PGO" where you instrument a binary with ',(0,s.jsx)(i.code,{children:"bolt"})," to collect profiling data. Run that binary. Then finally reorganize the binary layout using the collected profile data. This generally works better on large statically linked binaries but smaller binaries or libraries such as found in a typical package can benefit too. This optimization is still quite new."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Regardless, that's enough word spaghetti, let's look at the process to actually optimize a package."}),"\n",(0,s.jsx)(i.h1,{id:"optimizing-a-package",children:"Optimizing a Package"}),"\n",(0,s.jsxs)(i.p,{children:["Right, to begin with we'll have to start on choosing an actual package to benchmark and optimize. I've heard the ",(0,s.jsx)(i.code,{children:".webp"})," file format is becoming increasingly common on the web, slowly replacing ",(0,s.jsx)(i.code,{children:".png"})," and ",(0,s.jsx)(i.code,{children:".jpg"})," file formats due to the strong backing of Google (for better or for worst). An improvement in decoding time for ",(0,s.jsx)(i.code,{children:".webp"})," files would benefit any user using a web browser casually browsing the web."]}),"\n",(0,s.jsxs)(i.p,{children:["Let's have a look at the ",(0,s.jsx)(i.code,{children:"package.yml"})," build recipe for ",(0,s.jsx)(i.code,{children:"libwebp"}),"."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-yaml",children:"name       : libwebp\nversion    : 1.3.2\nrelease    : 25\nsource     :\n    - https://github.com/webmproject/libwebp/archive/refs/tags/v1.3.2.tar.gz : c2c2f521fa468e3c5949ab698c2da410f5dce1c5e99f5ad9e70e0e8446b86505\nhomepage   : https://developers.google.com/speed/webp/\nlicense    : BSD-3-Clause\ncomponent  : multimedia.codecs\nsummary    : A new image format for the web\ndescription: |\n    WebP is a new image format that provides lossless and lossy compression for images on the web. WebP lossless images are 26% smaller in size compared to PNGs. WebP lossy images are 25-34% smaller in size compared to JPEG images at equivalent SSIM index. WebP supports lossless transparency (also known as alpha channel) with just 22% additional bytes. Transparency is also supported with lossy compression and typically provides 3x smaller file sizes compared to PNG when lossy compression is acceptable for the red/green/blue color channels.\nemul32     : yes\npatterns   :\n    - devel : /usr/share/man\nbuilddeps  :\n    - pkgconfig32(glu)\n    - pkgconfig32(glut)\n    - pkgconfig32(libpng)\n    - pkgconfig32(libtiff-4)\n    - pkgconfig32(libturbojpeg)\n    - pkgconfig32(zlib)\n    - giflib-devel\nsetup      : |\n    %reconfigure --disable-static --enable-everything\nbuild      : |\n    %make\ninstall    : |\n    %make_install\n"})}),"\n",(0,s.jsxs)(i.p,{children:["Okay, looks to be a quite simple affair. A simple configure, make, make install as well as ",(0,s.jsx)(i.code,{children:"emul32"})," being enabled specifying the -32bit packages are also provided from this recipe. Next step is to look for a repeatable and easy way to benchmark it. We'll begin by looking at the ",(0,s.jsx)(i.code,{children:"pspec_x86_64.xml"})," file which lists all the files produced from the ",(0,s.jsx)(i.code,{children:"package.yml"})," recipe as well as some metadata."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-xml",children:'        <Name>libwebp</Name>\n        <Summary xml:lang="en">A new image format for the web</Summary>\n        <Description xml:lang="en">WebP is a new image format that provides lossless and lossy compression for images on the web. WebP lossless images are 26% smaller in size compared to PNGs. WebP lossy images are 25-34% smaller in size compared to JPEG images at equivalent SSIM index. WebP supports lossless transparency (also known as alpha channel) with just 22% additional bytes. Transparency is also supported with lossy compression and typically provides 3x smaller file sizes compared to PNG when lossy compression is acceptable for the red/green/blue color channels.\n</Description>\n        <PartOf>multimedia.codecs</PartOf>\n        <Files>\n            <Path fileType="executable">/usr/bin/cwebp</Path>\n            <Path fileType="executable">/usr/bin/dwebp</Path>\n            <Path fileType="executable">/usr/bin/gif2webp</Path>\n            <Path fileType="executable">/usr/bin/img2webp</Path>\n            <Path fileType="executable">/usr/bin/vwebp</Path>\n            <Path fileType="executable">/usr/bin/webpinfo</Path>\n            <Path fileType="executable">/usr/bin/webpmux</Path>\n'})}),"\n",(0,s.jsxs)(i.p,{children:["Perfect, we have ",(0,s.jsx)(i.code,{children:"dwebp"})," and ",(0,s.jsx)(i.code,{children:"cwebp"})," binaries available in the main package, which from a guess can be used to decode and encode ",(0,s.jsx)(i.code,{children:".webp"})," files. Let's try it out."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:"$ dwebp -h\nUsage: dwebp in_file [options] [-o out_file]\n\nDecodes the WebP image file to PNG format [Default].\nNote: Animated WebP files are not supported.\n$ cwebp -h\nUsage:\n\n   cwebp [options] -q quality input.png -o output.webp\n"})}),"\n",(0,s.jsxs)(i.p,{children:["Awesome, these binaries do exactly what we need to benchmark ",(0,s.jsx)(i.code,{children:"libwebp"}),", but, we are also indirectly testing ",(0,s.jsx)(i.code,{children:"libpng"})," as well for this benchmark, we'll have to keep an eye out for that."]}),"\n",(0,s.jsxs)(i.p,{children:["One extra step we have to do is ensure these binaries are actually linking against their own library, as upstream developers can have a habit of making sure their binaries don't link against their own libraries and end up being self-contained. Run ",(0,s.jsx)(i.code,{children:"ldd"})," to verify."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:"$ ldd /usr/bin/dwebp\n\tlinux-vdso.so.1 (0x00007ffed8733000)\n\tlibwebpdemux.so.2 => /usr/lib/libwebpdemux.so.2.0.14 (0x00007f7473bb4000)\n\tlibwebp.so.7 => /usr/lib/libwebp.so.7.1.8 (0x00007f7473ae2000)\n\tlibpng16.so.16 => /usr/lib/libpng16.so.16 (0x00007f7473aa6000)\n\tlibc.so.6 => /usr/lib/glibc-hwcaps/x86-64-v3/libc.so.6 (0x00007f74738a9000)\n\tlibsharpyuv.so.0 => /usr/lib/libsharpyuv.so.0.0.1 (0x00007f747389e000)\n\tlibm.so.6 => /usr/lib/glibc-hwcaps/x86-64-v3/libm.so.6 (0x00007f74737b8000)\n\tlibz.so.1 => /usr/lib/libz.so.1.3.0 (0x00007f7473200000)\n\t/usr/lib64/ld-linux-x86-64.so.2 (0x00007f7473bea000)\n"})}),"\n",(0,s.jsxs)(i.p,{children:["Awesome in this case both ",(0,s.jsx)(i.code,{children:"dwebp"})," and ",(0,s.jsx)(i.code,{children:"cwebp"})," link against ",(0,s.jsx)(i.code,{children:"libwebp.so"})," so we can be confident that any performance improvements will be applicable to all packages in the repository linking against ",(0,s.jsx)(i.code,{children:"libwebp"}),"."]}),"\n",(0,s.jsxs)(i.p,{children:["Let's grab a couple of ",(0,s.jsx)(i.code,{children:".webp"})," files from ",(0,s.jsx)(i.a,{href:"https://developers.google.com/speed/webp/gallery1",children:"here"})," to test with. We'll just use the largest image size available in this case to reduce noise as much as possible when running benchmarks as well as allow any potential optimizations to stretch their legs a little more."]}),"\n",(0,s.jsxs)(i.p,{children:["Now having done the prep work, lets actually benchmark the damn thing using ",(0,s.jsx)(i.code,{children:"hyperfine"})," for the time being."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:'$ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):     202.2 ms \xb1   0.3 ms    [User: 198.9 ms, System: 3.0 ms]\n  Range (min \u2026 max):   201.8 ms \u2026 202.7 ms    14 runs\n$ hyperfine "cwebp ~/PNG_Test.png -o /dev/null"\nBenchmark 1: cwebp ~/PNG_Test.png -o /dev/null\n  Time (mean \xb1 \u03c3):      1.423 s \xb1  0.009 s    [User: 1.346 s, System: 0.076 s]\n  Range (min \u2026 max):    1.410 s \u2026  1.435 s    10 runs\n'})}),"\n",(0,s.jsx)(i.p,{children:"There, we have our a basic baseline for encode and decode performance. We mostly care about decode performance here but improved encoding performance would also not go amiss."}),"\n",(0,s.jsx)(i.h2,{id:"lets-start-obvious",children:"Let's Start Obvious"}),"\n",(0,s.jsxs)(i.p,{children:["Let's start basic, enabling the ",(0,s.jsx)(i.code,{children:"speed"})," optimization which basically builds with ",(0,s.jsx)(i.code,{children:"-O3"})," instead of ",(0,s.jsx)(i.code,{children:"-O2"})," as well as any other flags that are deemed to be worthy to be part of the ",(0,s.jsx)(i.code,{children:"speed"})," group. As well as, the ",(0,s.jsx)(i.code,{children:"lto"})," optimization which builds with link time optimization allowing for inter-procedural optimizations to take place."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-yaml",children:"optimize:\n    - speed\n    - lto\n"})}),"\n",(0,s.jsx)(i.p,{children:"Moment of truth..."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:'$ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):     200.0 ms \xb1   1.5 ms    [User: 197.3 ms, System: 2.5 ms]\n  Range (min \u2026 max):   198.1 ms \u2026 203.2 ms    15 runs\n$ hyperfine "cwebp ~/PNG_Test.png -o /dev/null"\nBenchmark 1: cwebp ~/PNG_Test.png -o /dev/null\n  Time (mean \xb1 \u03c3):      1.353 s \xb1  0.012 s    [User: 1.281 s, System: 0.071 s]\n  Range (min \u2026 max):    1.336 s \u2026  1.369 s    10 runs\n'})}),"\n",(0,s.jsx)(i.p,{children:"Well okay, we got a very minor uplift in decoding performance and a slightly higher improvement in encoding performance, but nothing too much to write home about. Luckily we have several more optimizations to explore..."}),"\n",(0,s.jsx)(i.h2,{id:"pgo-is-great-except-when-it-isnt",children:"PGO is great, except, when it isn't."}),"\n",(0,s.jsxs)(i.p,{children:["Next step is to explore PGO (Profile Guided Optimization). For our ",(0,s.jsx)(i.code,{children:"libwebp"})," package, looks like we already hit a bit of a snafu. There's no test suite included in the tarball! That's a bit of a disappointment as a test suite such as ",(0,s.jsx)(i.code,{children:"make check"})," is by far and away the easiest and most comprehensive workload that can be used for profiling as part of PGO, especially for smaller libraries. However, we can still experiment with the just built ",(0,s.jsx)(i.code,{children:"dwebp"})," and ",(0,s.jsx)(i.code,{children:"cwebp"})," binaries as a suitable workload for PGO."]}),"\n",(0,s.jsx)(i.p,{children:"Luckily, as part of the package.yml format all you have to do is provide a profile for automatic PGO. After chrooting into the build environment and fiddling around a bit we end up with:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-yaml",children:"profile    : |\n    ./examples/dwebp webp_js/test_webp_js.webp -o test_png.png\n    ./examples/cwebp test_png.png -o /dev/null\n"})}),"\n",(0,s.jsx)(i.p,{children:"After specifying that, 6 builds will now be performed instead of 2:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["emul32:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Instrumented build"}),"\n",(0,s.jsx)(i.li,{children:"Run profiling workload"}),"\n",(0,s.jsx)(i.li,{children:"Optimized build using profiling data"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["x86_64:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Instrumented build"}),"\n",(0,s.jsx)(i.li,{children:"Run profiling workload"}),"\n",(0,s.jsx)(i.li,{children:"Optimized build using profiling data"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"For this relatively small package it increases the build time from 1m1.672s to 1m42.199s"}),"\n",(0,s.jsx)(i.p,{children:"The next moment of truth..."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:'$ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):     204.1 ms \xb1   2.2 ms    [User: 201.3 ms, System: 2.7 ms]\n  Range (min \u2026 max):   201.6 ms \u2026 207.1 ms    14 runs\n$ hyperfine "cwebp ~/PNG_Test.png -o /dev/null"\nBenchmark 1: cwebp ~/PNG_Test.png -o /dev/null\n  Time (mean \xb1 \u03c3):      1.349 s \xb1  0.010 s    [User: 1.266 s, System: 0.082 s]\n  Range (min \u2026 max):    1.335 s \u2026  1.374 s    10 runs\n'})}),"\n",(0,s.jsxs)(i.p,{children:["Well... That's interesting. We actually regress in performance for decode performance whilst gaining another small bump in encoding performance. Worse still, we get a bunch of ",(0,s.jsx)(i.code,{children:"profile count data file not found [-Wmissing-profile]"})," warning messages during the optimized build indicating to us our profiling workload isn't comprehensive enough and doesn't cover enough code paths. The lack of a handy ",(0,s.jsx)(i.code,{children:"make check"})," that could be used as a profiling workload is really hurting us here. For now, let's put a pin in exploring PGO, it isn't a dead end but more work needs to be done curating a more comprehensive workload to chuck at it in this particular case, whilst other, easier, optimization techniques are still available to us."]}),"\n",(0,s.jsx)(i.h2,{id:"256-vector-units-go-brrrrrr",children:"256 Vector Units go brrrrrr..."}),"\n",(0,s.jsxs)(i.p,{children:["The next obvious step is to explore ",(0,s.jsx)(i.code,{children:"glibc"})," hardware capabilities. For those unaware both ",(0,s.jsx)(i.code,{children:"clang"})," and ",(0,s.jsx)(i.code,{children:"gnu"})," compilers provide ",(0,s.jsx)(i.code,{children:"x86_64-v2"}),", ",(0,s.jsx)(i.code,{children:"x86_64-v3"})," and ",(0,s.jsx)(i.code,{children:"x86_64-v4"})," micro-architecture build options on top of the baseline of ",(0,s.jsx)(i.code,{children:"x86_64"}),". These enable the use of targeting additional CPU instruction sets during compilation for better performance. For example, ",(0,s.jsx)(i.code,{children:"-sse4.2"})," for ",(0,s.jsx)(i.code,{children:"x86_64-v2"}),", ",(0,s.jsx)(i.code,{children:"-avx2"})," for ",(0,s.jsx)(i.code,{children:"x86_64-v3"}),", and ",(0,s.jsx)(i.code,{children:"-avx512"})," for ",(0,s.jsx)(i.code,{children:"x86_64-v4"}),"."]}),"\n",(0,s.jsxs)(i.p,{children:["Whilst that's great 'n all, if a program is built with ",(0,s.jsx)(i.code,{children:"x86_64-v3"})," and gains an additional ~10% uplift in performance, it's no good if a ",(0,s.jsx)(i.code,{children:"x86_64-v2"})," compatible cpu can't run it. Luckily the ",(0,s.jsx)(i.code,{children:"glibc"})," loader that's found on almost general purpose linux installs provides a way to load higher or lower micro-architecture libraries if they're installed and supported."]}),"\n",(0,s.jsxs)(i.p,{children:["On top of all that, the ",(0,s.jsx)(i.code,{children:"package.yml"})," format provides an incredibly simple way of providing ",(0,s.jsx)(i.code,{children:"x86_64-v3"})," built libraries by enabling the ",(0,s.jsx)(i.code,{children:"avx2 : yes"})," flag."]}),"\n",(0,s.jsxs)(i.p,{children:["With ",(0,s.jsx)(i.code,{children:"avx2 : yes"})," enabled in the ",(0,s.jsx)(i.code,{children:"libwebp"})," package three builds are performed."]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"emul32"}),"\n",(0,s.jsx)(i.li,{children:"x86_64-v3"}),"\n",(0,s.jsx)(i.li,{children:"x86_64"}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["We now see these additional files in the ",(0,s.jsx)(i.code,{children:"pspec_x86_64.xml"})," file"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-diff",children:'+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libsharpyuv.so.0</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libsharpyuv.so.0.0.1</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebp.so.7</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebp.so.7.1.8</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpdecoder.so.3</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpdecoder.so.3.1.8</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpdemux.so.2</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpdemux.so.2.0.14</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpmux.so.3</Path>\n+            <Path fileType="library">/usr/lib64/glibc-hwcaps/x86-64-v3/libwebpmux.so.3.0.13</Path>\n'})}),"\n",(0,s.jsxs)(i.p,{children:["Let's rerun ",(0,s.jsx)(i.code,{children:"lld"})," on ",(0,s.jsx)(i.code,{children:"dwebp"})," after installing the new package and..."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:"$ ldd /usr/bin/dwebp\n\tlinux-vdso.so.1 (0x00007ffeab5b1000)\n\tlibwebpdemux.so.2 => /usr/lib/glibc-hwcaps/x86-64-v3/libwebpdemux.so.2.0.14 (0x00007f9a351d5000)\n\tlibwebp.so.7 => /usr/lib/glibc-hwcaps/x86-64-v3/libwebp.so.7.1.8 (0x00007f9a3510b000)\n\tlibpng16.so.16 => /usr/lib/libpng16.so.16 (0x00007f9a350cf000)\n\tlibc.so.6 => /usr/lib/glibc-hwcaps/x86-64-v3/libc.so.6 (0x00007f9a34ed2000)\n\tlibsharpyuv.so.0 => /usr/lib/glibc-hwcaps/x86-64-v3/libsharpyuv.so.0.0.1 (0x00007f9a34ec1000)\n\tlibm.so.6 => /usr/lib/glibc-hwcaps/x86-64-v3/libm.so.6 (0x00007f9a34ddb000)\n\tlibz.so.1 => /usr/lib/glibc-hwcaps/x86-64-v3/libz.so.1.3 (0x00007f9a34dbb000)\n\t/usr/lib64/ld-linux-x86-64.so.2 (0x00007f9a3520b000)\n"})}),"\n",(0,s.jsxs)(i.p,{children:["We can crucially see that ",(0,s.jsx)(i.code,{children:"dwebp"})," is now loading the ",(0,s.jsx)(i.code,{children:"x86-64-v3"})," built ",(0,s.jsx)(i.code,{children:"libwebp.so"})," lib from ",(0,s.jsx)(i.code,{children:"/usr/lib/glibc-hwcaps/x86-64-v3/libwebp.so.7.1.8"}),", success! Let's what our performance looks like."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:'$ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):     198.2 ms \xb1   1.2 ms    [User: 195.4 ms, System: 2.5 ms]\n  Range (min \u2026 max):   197.0 ms \u2026 200.5 ms    14 runs\n$ hyperfine "cwebp ~/PNG_Test.png -o /dev/null"\nBenchmark 1: cwebp ~/PNG_Test.png -o /dev/null\n  Time (mean \xb1 \u03c3):      1.313 s \xb1  0.009 s    [User: 1.243 s, System: 0.078 s]\n  Range (min \u2026 max):    1.308 s \u2026  1.341 s    10 runs\n'})}),"\n",(0,s.jsx)(i.p,{children:"Let's recap so far:"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Optimization"}),(0,s.jsx)(i.th,{children:"Decode"}),(0,s.jsx)(i.th,{children:"Encode"}),(0,s.jsx)(i.th,{children:"Size"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Baseline"}),(0,s.jsx)(i.td,{children:"202.2 ms"}),(0,s.jsx)(i.td,{children:"1.399 s"}),(0,s.jsx)(i.td,{children:"1.33 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO"}),(0,s.jsx)(i.td,{children:"200.0 ms"}),(0,s.jsx)(i.td,{children:"1.353 s"}),(0,s.jsx)(i.td,{children:"1.73 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + PGO"}),(0,s.jsxs)(i.td,{children:["204.1 ms ","\u26a0\ufe0f"]}),(0,s.jsx)(i.td,{children:"1.349 s"}),(0,s.jsx)(i.td,{children:"1.07 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + x86_64-v3"}),(0,s.jsx)(i.td,{children:"198.2 ms"}),(0,s.jsx)(i.td,{children:"1.313 s"}),(0,s.jsx)(i.td,{children:"3.17 MB"})]})]})]}),"\n",(0,s.jsxs)(i.p,{children:["Whilst we're still getting an additional speedup it isn't really anything to write home about. A measly ~2% improvement in decoding performance and a fairly respectable ~7% improvement in encoding performance for our test case. However, increasing the package size by an extra ~2MiB from providing a bunch of extra libs in ",(0,s.jsx)(i.code,{children:"/usr/lib/glibc-hwcaps/x86-64-v3/"})," just ain't worth it. This hints that either the compiler optimizations aren't really effective here or we're being bottle-necked by something else."]}),"\n",(0,s.jsxs)(i.p,{children:["So far, we've been benchmarking fairly simply using ",(0,s.jsx)(i.code,{children:"hyperfine"}),", let's swap that out for ",(0,s.jsx)(i.code,{children:"perf"})," so we can get a callgraph."]}),"\n",(0,s.jsxs)(i.h2,{id:"something-about-perf-to-the-rescue",children:["Something about ",(0,s.jsx)(i.code,{children:"perf"})," to the rescue"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.code,{children:"perf record -o dwebp.data -- dwebp ~/3.webp -o /dev/null"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.code,{children:"perf record -o cwebp.data -- cwebp ~/PNG_Test.png -o /dev/null"})}),"\n",(0,s.jsxs)(i.p,{children:["Let's look at ",(0,s.jsx)(i.code,{children:"dwebp"})," first with ",(0,s.jsx)(i.code,{children:"perf report -i dwebp.data"}),"."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Perf report dwebp",src:n(58916).Z+"",width:"1254",height:"676"})}),"\n",(0,s.jsxs)(i.p,{children:["Well god damn, literally all of our time is being spent in ",(0,s.jsx)(i.code,{children:"libz.so"})," it's no wonder our compiler optimizations were hardly improving performance."]}),"\n",(0,s.jsxs)(i.p,{children:["Let's also look at the ",(0,s.jsx)(i.code,{children:"cwebp"})," report, we've generally been getting better results from it."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Perf report cwebp",src:n(85370).Z+"",width:"1254",height:"676"})}),"\n",(0,s.jsxs)(i.p,{children:["Okay, much more of our time is being spent in ",(0,s.jsx)(i.code,{children:"libwebp.so"})," itself here which helps to explain why we were seeing a better performance uplift. Still 5.68% of our time is being spent in ",(0,s.jsx)(i.code,{children:"libz"}),"."]}),"\n",(0,s.jsx)(i.h2,{id:"choosing-wisely",children:"Choosing Wisely"}),"\n",(0,s.jsxs)(i.p,{children:["You may remember early on, when I said we are also indirectly testing ",(0,s.jsx)(i.code,{children:"libpng"}),". Well... after some more digging, that's exactly what's happening here. Re-running the ",(0,s.jsx)(i.code,{children:"dwebp"})," binary it says this"]}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsx)(i.p,{children:"Decodes the WebP image file to PNG format"}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["Turns out, it's more accurate to say we are ",(0,s.jsx)(i.em,{children:"directly"})," testing ",(0,s.jsx)(i.code,{children:"libpng"})," and by extension ",(0,s.jsx)(i.code,{children:"zlib"}),". It isn't ",(0,s.jsx)(i.code,{children:"libwebp"})," that's spending all of its time in ",(0,s.jsx)(i.code,{children:"libz.so"}),", it's ",(0,s.jsx)(i.code,{children:"libpng"}),"! This is exactly the reason you have to be careful about the benchmarks chosen and, ensure you understand what they're doing."]}),"\n",(0,s.jsx)(i.p,{children:"However, the good news about this little snafu is:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.code,{children:"dwebp"})," can be used to translate to another image format such as ",(0,s.jsx)(i.code,{children:".yuv"})," that'll more accurately remove the bottleneck from ",(0,s.jsx)(i.code,{children:"libz.so"}),"."]}),"\n",(0,s.jsxs)(i.li,{children:["We now know that ",(0,s.jsx)(i.code,{children:"libpng"})," has a huge bottleneck in ",(0,s.jsx)(i.code,{children:"libz.so"})," and speeding up ",(0,s.jsx)(i.code,{children:"zlib"})," ",(0,s.jsx)(i.em,{children:"should"})," dramatically speed up ",(0,s.jsx)(i.code,{children:"libpng"})," performance."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"adjusting-the-benchmark",children:"Adjusting the Benchmark"}),"\n",(0,s.jsxs)(i.p,{children:["After reverting the ",(0,s.jsx)(i.code,{children:"libwebp"})," package to the baseline let's use our adjusted decoding benchmark."]}),"\n",(0,s.jsxs)(i.p,{children:["We'll now use ",(0,s.jsx)(i.code,{children:"dwebp ~/3.webp -yuv -o /dev/null"})," for a decoding test, let's run that with ",(0,s.jsx)(i.code,{children:"perf"})," to ensure we're exclusively testing ",(0,s.jsx)(i.code,{children:"libwebp.so"})," here."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Perf report dwebp to yuv",src:n(29503).Z+"",width:"1254",height:"676"})}),"\n",(0,s.jsxs)(i.p,{children:["Okay that's awesome, no ",(0,s.jsx)(i.code,{children:"libpng.so"})," or ",(0,s.jsx)(i.code,{children:"libz.so"})," to mess with our tests!"]}),"\n",(0,s.jsx)(i.p,{children:"Let's reapply our optimizations, keeping those which apply an uplift"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Optimization"}),(0,s.jsx)(i.th,{children:"Decode"}),(0,s.jsx)(i.th,{children:"Size"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Baseline"}),(0,s.jsx)(i.td,{children:"14.7 ms"}),(0,s.jsx)(i.td,{children:"1.33 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed"}),(0,s.jsx)(i.td,{children:"14.5 ms"}),(0,s.jsx)(i.td,{children:"1.56 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"LTO"}),(0,s.jsx)(i.td,{children:"14.6 ms"}),(0,s.jsx)(i.td,{children:"1.40 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"PGO"}),(0,s.jsxs)(i.td,{children:["18.0 ms ","\u26a0\ufe0f"]}),(0,s.jsx)(i.td,{children:"1.07 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"x86-64-v3"}),(0,s.jsx)(i.td,{children:"12.7 ms"}),(0,s.jsx)(i.td,{children:"2.35 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + x86-64-v3"}),(0,s.jsx)(i.td,{children:"12.3 ms"}),(0,s.jsx)(i.td,{children:"3.17 MB"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:"Okay, this is great, whilst we aren't getting much from speed or LTO, we are getting a big uplift from x86-64-v3 libraries and when combined with the other optimizations we're getting an uplift in performance of around ~16% at the cost of close to thrice the installed package size."}),"\n",(0,s.jsx)(i.h3,{id:"partial-profiling",children:"Partial Profiling"}),"\n",(0,s.jsxs)(i.p,{children:["Once again we see that PGO regresses performance hard, however, that smaller size is giving a good hint! We already know that the profiling workload we gave it isn't very comprehensive due to the bunch of ",(0,s.jsx)(i.code,{children:"-Wmissing-profile"})," warnings we get during the optimized build. By default, PGO will aggressively inline and apply additional optimizations to code that's part of the profiling workload while everything else will be optimized for size. The idea being, hot-path code is fast and code that doesn't matter is small. However, what happens when you can't craft a comprehensive workload, as seems to be the case here? Luckily GCC has a flag for exactly that ",(0,s.jsx)(i.code,{children:"-fprofile-partial-training"}),". GCC docs state that:"]}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsx)(i.p,{children:"In some cases it is not practical to train all possible hot paths in the program. (For example, program may contain functions specific for a given hardware and training may not cover all hardware configurations program is run on.) With -fprofile-partial-training profile feedback will be ignored for all functions not executed during the train run leading them to be optimized as if they were compiled without profile feedback. This leads to better performance when train run is not representative but also leads to significantly bigger code."}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["Okay, let's try it out, all we need to do is specify in our ",(0,s.jsx)(i.code,{children:"package.yml"})," recipe."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-yaml",children:'environment: |\n    if [[ -n ${PGO_USE_BUILD} ]]; then\n        export CFLAGS="${CFLAGS} -fprofile-partial-training"\n        export CXXFLAGS="${CXXFLAGS} -fprofile-partial-training"\n    fi\n'})}),"\n",(0,s.jsx)(i.p,{children:"And the results:"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Optimization"}),(0,s.jsx)(i.th,{children:"Decode"}),(0,s.jsx)(i.th,{children:"Size"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + x86-64-v3"}),(0,s.jsx)(i.td,{children:"12.3 ms"}),(0,s.jsx)(i.td,{children:"3.17 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + x86-64-v3 + Partial PGO"}),(0,s.jsx)(i.td,{children:"12.5 ms"}),(0,s.jsx)(i.td,{children:"3.13 MB"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:"Well, it was worth a try. This highlights how useless PGO can be when you don't or can't provide it a good workload. Interestingly, we don't get the size bloat that was promised, in fact, the opposite."}),"\n",(0,s.jsx)(i.h1,{id:"final-libwebp-results",children:"Final libwebp Results"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Benchmark"}),(0,s.jsx)(i.th,{children:"Time Before"}),(0,s.jsx)(i.th,{children:"Time After"}),(0,s.jsx)(i.th,{children:"Size Before"}),(0,s.jsx)(i.th,{children:"Size After"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:'"dwebp ~/3.webp -yuv -o /dev/null"'}),(0,s.jsx)(i.td,{children:"14.5 ms"}),(0,s.jsx)(i.td,{children:"12.3 ms"}),(0,s.jsx)(i.td,{children:"1.33 MB"}),(0,s.jsx)(i.td,{children:"3.17 MB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:'"cwebp ~/PNG_Test.png -o /dev/null"'}),(0,s.jsx)(i.td,{children:"1.399 s"}),(0,s.jsx)(i.td,{children:"1.313 s"}),(0,s.jsx)(i.td,{children:"--"}),(0,s.jsx)(i.td,{children:"--"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:"In the end, we get a very healthy ~16% improvement in decoding from a .webp to .yuv file. As well as a respectable 6% improvement in encoding from a .png to .webp file. However, the increased package size is very unfortunate. It's possible to tweak the x86-64-v3 build and only ship the libs that actually improve performance in order to get the installed size back to an acceptable level."}),"\n",(0,s.jsx)(i.h1,{id:"next-generation-side-quest",children:'"Next-Generation" Side Quest'}),"\n",(0,s.jsxs)(i.p,{children:["Now, you probably remember earlier due to our unrepresentative benchmark we found out that ",(0,s.jsx)(i.code,{children:"libpng"})," is getting highly bottlenecked by ",(0,s.jsx)(i.code,{children:"libz.so"}),". This now seems like a perfect opportunity to take a look at zlib and circle-back to our original benchmark that we were using."]}),"\n",(0,s.jsxs)(i.p,{children:["Zlib is widely employed throughout the ecosystem and, as such you'd think it would be highly-optimized for performance. However, that isn't really the case. Zlib is written in an old-fashioned way of C and tries to be ",(0,s.jsx)(i.em,{children:"extremely"})," portable; supporting dozens of systems that have fallen out of common use. As such, it's hard to apply architecture specific optimizations that wouldn't break some old system or without introducing code spaghetti. There have been a couple of attempts to merge AArch64 and x86_64 optimizations into the canonical zlib library without success."]}),"\n",(0,s.jsxs)(i.p,{children:["However, there is some light in this tunnel as various forks of zlib having been popping up, applying new optimizations on top of zlib. The most promising of these looks be to ",(0,s.jsx)(i.a,{href:"https://github.com/zlib-ng/zlib-ng/",children:"zlib-ng"}),". When built in compatible mode, it promises to be API compatible with canonical zlib and ",(0,s.jsx)(i.em,{children:"tries"})," to be as ABI compatible as possible."]}),"\n",(0,s.jsxs)(i.p,{children:["Let's just go for it, replacing Solus' ",(0,s.jsx)(i.code,{children:"zlib"})," package with zlib-ng built in compatible mode. It's a bit scary due to how integral zlib is in a typical Linux install, but, how hard could it be?"]}),"\n",(0,s.jsx)(i.h2,{id:"i-zee-a-purty-lil-package",children:"I Zee a Purty lil' Package"}),"\n",(0,s.jsxs)(i.p,{children:["Well that was simple. Here's what our zlib-ng ",(0,s.jsx)(i.code,{children:"package.yml"})," recipe looks like."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-yaml",children:"name       : zlib\nversion    : 2.1.5\nrelease    : 28\nsource     :\n    - https://github.com/zlib-ng/zlib-ng/archive/refs/tags/2.1.5.tar.gz : 3f6576971397b379d4205ae5451ff5a68edf6c103b2f03c4188ed7075fbb5f04\nhomepage   : https://github.com/zlib-ng/zlib-ng\nlicense    : ZLIB\ncomponent  : system.base\nsummary    : zlib replacement with optimizations for next generation systems.\ndescription:\n    - A zlib data compression library for the next generation systems. ABI/API compatible mode.\ndevel      : yes\nemul32     : yes\nsetup      : |\n    %cmake_ninja \\\n        -DZLIB_COMPAT=ON \\\n        -DWITH_GTEST=OFF \\\n        -DBUILD_SHARED_LIBS=ON \\\n        -DINSTALL_LIB_DIR=%libdir%\nbuild      : |\n    %ninja_build\ninstall    : |\n    %ninja_install\ncheck      : |\n    %ninja_check\n"})}),"\n",(0,s.jsxs)(i.p,{children:["After building it, all the files seem to be in the right place and the test suite is passing. Let's just install it overwriting our canonical ",(0,s.jsx)(i.code,{children:"zlib"})," package and hope our system doesn't die... I think the word is YOLO."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:'$ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):     198.6 ms \xb1   2.3 ms    [User: 194.3 ms, System: 3.6 ms]\n  Range (min \u2026 max):   196.3 ms \u2026 203.3 ms    14 runs\n$ sudo eopkg it zlib-2.1.5-28-1-x86_64.eopkg\n...\n$ $ hyperfine "dwebp ~/3.webp -o /dev/null"\nBenchmark 1: dwebp ~/3.webp -o /dev/null\n  Time (mean \xb1 \u03c3):      87.6 ms \xb1   0.7 ms    [User: 84.7 ms, System: 2.6 ms]\n  Range (min \u2026 max):    86.5 ms \u2026  88.7 ms    33 runs\n'})}),"\n",(0,s.jsx)(i.p,{children:"Well, hot diggity damn. Swapping out the zlib package for a more performant variant has instantly more than halved(!!) our decoding time."}),"\n",(0,s.jsxs)(i.p,{children:["We need to find a more contained ",(0,s.jsx)(i.code,{children:"libpng"})," benchmark here that removes the ",(0,s.jsx)(i.code,{children:"libwebp"})," stuff to really confirm the findings here. After some sleuthing the ",(0,s.jsx)(i.code,{children:"libpng"})," source repository has a ",(0,s.jsx)(i.a,{href:"https://github.com/glennrp/libpng/blob/libpng16/contrib/examples/pngtopng.c",children:"pngtopng.c"})," file we can compile to use the system libpng library."]}),"\n",(0,s.jsxs)(i.p,{children:["Changing the header from ",(0,s.jsx)(i.code,{children:'#include "../../png.h"'})," to ",(0,s.jsx)(i.code,{children:"#include <png.h>"})," then running ",(0,s.jsx)(i.code,{children:"gcc -Ofast pngtopng.c -lpng16 -o pngtopng"})," to compile it, we have a libpng benchmark. We can reuse our test .png file from earlier. Ending up with: ",(0,s.jsx)(i.code,{children:"./pngtopng ~/PNG_Test.png /dev/null"})," for our benchmark."]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Library"}),(0,s.jsx)(i.th,{children:"Time"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"zlib (canonical)"}),(0,s.jsx)(i.td,{children:"1.464 s"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"zlib-ng (compat)"}),(0,s.jsx)(i.td,{children:"896.6 ms"})]})]})]}),"\n",(0,s.jsxs)(i.p,{children:["Well. This is pretty much inline with our flawed ",(0,s.jsx)(i.code,{children:"dwebp"})," benchmark from earlier. Swapping out zlib almost halves ",(0,s.jsx)(i.code,{children:"libpng"})," decoding time."]}),"\n",(0,s.jsx)(i.h2,{id:"squeezing-more-from-zlib-ng",children:"Squeezing more from zlib-ng"}),"\n",(0,s.jsxs)(i.p,{children:["However, we're not done yet. We still have our compiler optimizations available to us to squeeze more performance from ",(0,s.jsx)(i.code,{children:"zlib-ng"}),"."]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Optimization"}),(0,s.jsx)(i.th,{children:"Decode"}),(0,s.jsx)(i.th,{children:"Size"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Baseline"}),(0,s.jsx)(i.td,{children:"896.6 ms"}),(0,s.jsx)(i.td,{children:"141.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed"}),(0,s.jsx)(i.td,{children:"883.6 ms"}),(0,s.jsx)(i.td,{children:"182.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"LTO"}),(0,s.jsx)(i.td,{children:"892.7 ms"}),(0,s.jsx)(i.td,{children:"133.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"PGO"}),(0,s.jsx)(i.td,{children:"894.6 ms"}),(0,s.jsx)(i.td,{children:"141.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"x86-64-v3"}),(0,s.jsx)(i.td,{children:"892.5 ms"}),(0,s.jsx)(i.td,{children:"295.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO"}),(0,s.jsx)(i.td,{children:"882.6 ms"}),(0,s.jsx)(i.td,{children:"170.00 KB"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Speed + LTO + PGO + x86-64-v3"}),(0,s.jsx)(i.td,{children:"882.5 ms"}),(0,s.jsx)(i.td,{children:"250.00 KB"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:"It looks like in this case the simple speed + LTO optimizations is the way to go. Speed gives the majority of the speedup but LTO helps bring back down the package size again. However, it's only a 1.5% improvement from baseline for this benchmark. We can always re-benchmark it later, testing zlib performance more directly instead of via libpng. It shows how good a job the zlib-ng developers have done that it's so performant right out of the gate."}),"\n",(0,s.jsx)(i.h1,{id:"final-words",children:"Final Words"}),"\n",(0,s.jsx)(i.p,{children:"We've shown the process of how a package can be optimized in Solus, through the failings and wins here I hope some good tips and tricks were provided in avoiding common pitfalls. Additional benchmarking strategies such as BOLT or Polly optimizations were not discussed and it'll be good material for a future blog post."}),"\n",(0,s.jsx)(i.p,{children:"Some other important things such as tweaking the system for benchmarking in order to get representative and consistent results were also not discussed. This is especially important in power budget constrained systems such as laptops and worth bearing in mind."}),"\n",(0,s.jsxs)(i.p,{children:["Regardless, I hope the story of how ",(0,s.jsx)(i.code,{children:"libwebp"})," was optimized for was entertaining and some things were learnt for anyone looking to optimize packages in Solus for the future."]})]})}function h(e={}){const{wrapper:i}={...(0,t.a)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},85370:(e,i,n)=>{n.d(i,{Z:()=>s});const s=n.p+"assets/images/perf_report_cwebp_png-baafb78e8e4527d10a5a1ded232c2bde.webp"},58916:(e,i,n)=>{n.d(i,{Z:()=>s});const s=n.p+"assets/images/perf_report_dwebp_png-c0f9529294b3efe6743a980866abdf15.webp"},29503:(e,i,n)=>{n.d(i,{Z:()=>s});const s=n.p+"assets/images/perf_report_dwebp_yuv-36ad3f6052746d5ea55acce6cbe72b6f.webp"},11151:(e,i,n)=>{n.d(i,{Z:()=>a,a:()=>r});var s=n(67294);const t={},l=s.createContext(t);function r(e){const i=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(l.Provider,{value:i},e.children)}}}]);